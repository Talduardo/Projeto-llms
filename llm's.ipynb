{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dfccb8b",
   "metadata": {},
   "source": [
    "## üîë Configura√ß√£o de Acesso via API (Modelos Propriet√°rios)\n",
    "Os modelos como GPT-4o (OpenAI), Claude 3.5 (Anthropic) e Gemini 1.5 (Google) geralmente exigem acesso via API. Aqui est√£o os passos para configurar:\n",
    "- Criar uma Conta\n",
    "- Acesse o site do provedor do modelo (exemplo: OpenAI, Anthropic, Google Cloud).\n",
    "- Registre-se e escolha um plano de uso (gratuito ou pago).\n",
    "- Obter a Chave de API\n",
    "- Ap√≥s o registro, v√° para a se√ß√£o de API Keys no painel de controle.\n",
    "- Gere uma chave de API exclusiva para autentica√ß√£o.\n",
    "- Instalar Bibliotecas Necess√°rias\n",
    "- Para Python, instale a biblioteca correspondente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai anthropic google-generativeai\n",
    "\n",
    "# Para Node.js, use:\n",
    "npm install openai anthropic google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454319b8",
   "metadata": {},
   "source": [
    "## - Autentica√ß√£o e Uso\n",
    "- No c√≥digo, insira a chave de API para autenticar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"SUA_CHAVE_AQUI\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explique contabilidade financeira\"}]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574e6b4",
   "metadata": {},
   "source": [
    "## - Gerenciamento de Tokens e Custos\n",
    "- Monitore o consumo de tokens no painel do provedor.\n",
    "- Configure limites para evitar cobran√ßas inesperadas\n",
    "\n",
    "## Configura√ß√£o de Acesso Local (Modelos Open-Source)\n",
    "Modelos como LLaMA, Falcon, Mistral, Mixtral podem ser executados localmente sem depender de APIs externas.\n",
    "- Baixar o Modelo\n",
    "- Acesse reposit√≥rios como Hugging Face ou GitHub para baixar o modelo desejado.\n",
    "- Exemplo para LLaMA:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ffda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/facebookresearch/llama\n",
    "\n",
    "# - Instalar Depend√™ncias\n",
    "# - Para rodar localmente, instale frameworks como llama.cpp ou Ollama.\n",
    "# - Exemplo de instala√ß√£o do Ollama:\n",
    "\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# - Executar o Modelo\n",
    "# - Carregue o modelo e inicie a infer√™ncia:\n",
    "\n",
    "ollama run mistral\n",
    "\n",
    "# - Acelera√ß√£o de Hardware\n",
    "# - Se tiver uma GPU NVIDIA, instale CUDA Toolkit para otimizar a execu√ß√£o.\n",
    "# - Configure o uso de GPU no c√≥digo\n",
    "\n",
    "import torch\n",
    "model = torch.load(\"llama_model.pth\").cuda()\n",
    "\n",
    "# - Privacidade e Controle\n",
    "# - Como os modelos s√£o locais, os dados n√£o s√£o enviados para servidores externos.\n",
    "# - Ideal para uso em ambientes corporativos com restri√ß√µes de seguran√ßa."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
